\chapter{\label{app:1-KL_divergence}Additional proofs}

\minitoc

\section{KL-divergence for non-overlapping mixture distributions}
\label{sec:kl_div}

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\begin{figure}[h!]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[every axis plot post/.append style={
  mark=none,samples=500,smooth,domain=-11:11},xlabel = {Posterior distribution}, legend entries = {$f_1$, $f_2$}, 
  axis x line*=bottom, 
  axis y line*=left, 
  enlargelimits=upper] 
  \addplot {gauss(-5,0.4)};
  \addplot {gauss(7.5,0.6)};
  \coordinate (a) at (axis cs:5, 0.1);
  \draw[red, dashed, thick](a |- current plot begin) -- (a);
  \coordinate (a) at (axis cs:10, 0.1);
  \draw[red, dashed, thick](a |- current plot begin) -- (a);
  \coordinate (a) at (axis cs:-10, 0.1);
  \draw[blue, dashed, thick](a |- current plot begin) -- (a);
  \coordinate (a) at (axis cs:0, 0.1);
  \draw[blue, dashed, thick](a |- current plot begin) -- (a);
\end{axis}
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[every axis plot post/.append style={
  mark=none,samples=500,smooth,domain=-11:11},xlabel = {Prior distribution}, legend entries = {$g_1$, $g_2$},
  axis x line*=bottom, 
  axis y line*=left, 
  enlargelimits=upper] 
  \addplot {gauss(-4,0.43)};
  \addplot {gauss(7,0.43)};
  \coordinate (a) at (axis cs:5, 0.1);
  \draw[red, dashed, thick](a |- current plot begin) -- (a);
  \coordinate (a) at (axis cs:10, 0.1);
  \draw[red, dashed, thick](a |- current plot begin) -- (a);
  \coordinate (a) at (axis cs:-10, 0.1);
  \draw[blue, dashed, thick](a |- current plot begin) -- (a);
  \coordinate (a) at (axis cs:0, 0.1);
  \draw[blue, dashed, thick](a |- current plot begin) -- (a);
\end{axis}
\end{tikzpicture}
\end{minipage}
\end{figure}
\textbf{Theorem 1:}
%
Let $q = \sum^{N}_i \pi_i f_i$ be a mixture of non-overlapping distributions, i.e., $S_i \cap S_j = \emptyset, \forall i \neq j$, where $S_i$ is the support of distribution $f_i$.
%
If $p$ is a probability distribution and can be represented as a mixture of non-overlapping distributions with the same support as $f_i$, i.e., $p = \sum^{N}_i \pi'_i g_i$ and support of $g_i$ = support of $f_i$, the KL divergence between $q$ and $p$ can be simplified as follows:

\begin{equation}
KL(q || p) = \sum^N_i \left( \pi_i KL (f_i || g_i) + \pi_i \log (\pi_i) - \pi_i \log(\pi'_i) \right)
\end{equation}

%

%
\textbf{Proof:} 
Assuming $q$ is $0$ outside $\cup^N_i S_i$, we can write the integral as

\begin{align}
    KL (q || p) &= \int q \log (q) - q \log (p) = \sum^N_i \int_{S_i} q \log (q) - q \log (p) \nonumber \\
    &= \sum^N_i \int_{S_i} \left(\sum^{N}_i \pi_i f_i \right)\log \left(\sum^{N}_i \pi_i f_i\right) - \left( \sum^{N}_i \pi_i f_i\right) \log \left(\sum^{N}_i \pi'_i g_i\right).
\end{align}

Because $S_i \cap S_j = \emptyset, \forall i \neq j$, we can simplify as

\begin{align}
   KL (q || p) &=  \sum^N_i \int_{S_i} \pi_i f_i \log(\pi_i f_i) - \pi_i f_i \log(\pi'_i g_i) \nonumber \\
   &= \sum^N_i \pi_i \left( \int_{S_i} f_i \log(f_i) -  f_i \log(g_i) \right) + \sum^N_i \left( \pi_i log (\pi_i) \int_{S_i} f_i - \pi_i log (\pi'_i) \int_{S_i} f_i \right) \nonumber \\
   &= \sum^N_i \pi_i \left( \int_{S_i} f_i \log(f_i) -  f_i \log(g_i) \right) + \sum^N_i \left(\pi_i log (\pi_i) - - \pi_i log (\pi'_i) \right) \nonumber \\
   &= \sum^N_i \left( \pi_i KL (f_i || g_i) + \pi_i \log (\pi_i) - \pi_i \log(\pi'_i) \right)
\end{align}

